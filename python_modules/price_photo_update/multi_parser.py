import os
import re
import time
import mysql.connector
import pandas as pd
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import random
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
OUTPUT_DIR = "output"
OUTPUT_FILENAME = "products.xlsx"
OUTPUT_PATH = os.path.join(OUTPUT_DIR, OUTPUT_FILENAME)
BASE_URL = "https://trast-zapchast.ru"
THREADS = 4  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–∞–ª–æ–≥–∞

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# –ü—Ä–æ–∫—Å–∏-—Å–µ—Ä–≤–µ—Ä—ã
PROXY_LIST = [
    "vpn-uk1.trafflink.xyz:443",
    "vpn-uk2.trafflink.xyz:443",
    "vpn-uk3.trafflink.xyz:443",
    "uk28.trafcfy.com:437",
    "uk27.trafcfy.com:437",
    "uk36.trafcfy.com:437",
]

current_proxy = None  # –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø—Ä–æ–∫—Å–∏ –¥–ª—è –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è

def get_random_proxy():
    return random.choice(PROXY_LIST)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö MySQL
def connect_to_db():
    try:
        return mysql.connector.connect(
            host="127.0.0.1",
            user="uploader",
            password="uploader",
            database="avito"
        )
    except mysql.connector.Error as err:
        logging.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {err}")
        raise

def update_config_status(db_connection, name, value):
    try:
        with db_connection.cursor() as cursor:
            cursor.execute("REPLACE INTO config (name, value) VALUES (%s, %s)", (name, value))
            db_connection.commit()
            logging.info(f"–°—Ç–∞—Ç—É—Å '{name}' —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω –¥–æ '{value}'")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—Ç–∞—Ç—É—Å–∞ '{name}': {e}")
        db_connection.rollback()

# –°–æ–∑–¥–∞–µ–º –±—Ä–∞—É–∑–µ—Ä —Å –≤—ã–±—Ä–∞–Ω–Ω—ã–º –ø—Ä–æ–∫—Å–∏
def create_driver(proxy):
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument(f"--proxy-server=https://{proxy}")
    options.add_argument("--log-level=3")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    logging.info(f'–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–∫—Å–∏: {proxy}')
    return driver

# –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω–∏—Ü –∏ –≤—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–∫—Å–∏
def get_total_pages():
    global current_proxy
    attempts = 3  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ —Å–º–µ–Ω—ã –ø—Ä–æ–∫—Å–∏
    for _ in range(attempts):
        proxy = get_random_proxy()
        driver = create_driver(proxy)
        driver.get(f"{BASE_URL}/shop/page/1")
        time.sleep(2)
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        pagination = soup.select('ul.page-numbers li')
        driver.quit()
        
        total_pages = int(pagination[-2].get_text(strip=True)) if pagination else 1
        if total_pages > 1:
            current_proxy = proxy
            break
        logging.warning(f'‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–∞ —Ç–æ–ª—å–∫–æ 1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞, —Å–º–µ–Ω–∞ –ø—Ä–æ–∫—Å–∏...')
    else:
        logging.error('‚ùå –ü–æ—Å–ª–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ø—ã—Ç–æ–∫ –≤—Å–µ –µ—â–µ —Ç–æ–ª—å–∫–æ 1 —Å—Ç—Ä–∞–Ω–∏—Ü–∞, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º –ø—Ä–æ–∫—Å–∏.')
    
    logging.info(f'–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–∫—Å–∏ {current_proxy} –¥–ª—è –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü.')
    return total_pages

# –§—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–∞–ª–æ–≥–∞
def parse_page(page_number):
    driver = create_driver(current_proxy)
    driver.get(f"{BASE_URL}/shop/page/{page_number}")
    time.sleep(2)
    
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    products = soup.find_all('div', class_='th-product-card')
    
    items = []
    count = 0
    for product in products:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª–∞—Å—Å–∞ "outofstock" –∏–ª–∏ —ç–ª–µ–º–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π —É–∫–∞–∑—ã–≤–∞–µ—Ç, —á—Ç–æ —Ç–æ–≤–∞—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        if product.find('a', class_='button product_type_variable'):
            continue
        
        try:
            name = product.find('div', class_='th-product-card__name').find('h2')
            name = name.get_text(strip=True) if name else '–ù/–î'
            
            price = product.find('div', class_='th-product-card__prices').find('span', class_='woocommerce-Price-amount')
            price = re.sub(r'[^\d]', '', price.get_text(strip=True)) if price else '0'
            
            article = product.find('div', class_='th-product-card__meta').find('span', class_='th-product-card__meta-value')
            article = re.sub(r'[\s\-]', '', article.get_text(strip=True)) if article else '–ù/–î'
            
            image_tag = product.find('div', class_='th-product-card__image').find('img')
            image_url = image_tag['src'] if image_tag else '–ù/–î'
            
            product_page_tag = product.find('a', class_='woocommerce-LoopProduct-link')
            product_page_link = product_page_tag['href'] if product_page_tag else '–ù/–î'
            
            manufacturer = "–ù/–î"
            
            items.append({'–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ': name, '–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å': manufacturer, '–ê—Ä—Ç–∏–∫—É–ª': article, '–¶–µ–Ω–∞': price, '–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ': image_url, '–°—Å—ã–ª–∫–∞': product_page_link})
            count += 1
            logging.info(f'‚úîÔ∏è –î–æ–±–∞–≤–ª–µ–Ω —Ç–æ–≤–∞—Ä: {name} | –ê—Ä—Ç–∏–∫—É–ª: {article} | –¶–µ–Ω–∞: {price}')
        except Exception as e:
            logging.warning(f'–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–æ–≤–∞—Ä–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_number}: {e}')
            continue
    
    driver.quit()
    logging.info(f'üìÑ –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_number}. –î–æ–±–∞–≤–ª–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {count}')
    return items

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
def main():
    with connect_to_db() as db_connection:
            try:
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–∞—á–∞–ª—å–Ω—ã–π —Å—Ç–∞—Ç—É—Å
                update_config_status(db_connection, "parser_status", "in_progress")
                total_pages = get_total_pages()
                logging.info(f'üîç –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–∞–Ω–∏—Ü: {total_pages}')
                
                all_items = []
                with ThreadPoolExecutor(max_workers=THREADS) as executor:
                    future_to_page = {executor.submit(parse_page, page): page for page in range(1, total_pages + 1)}
                    for future in as_completed(future_to_page):
                        all_items.extend(future.result())
                
                df = pd.DataFrame(all_items)
                df.to_excel(OUTPUT_PATH, index=False)
                logging.info(f'‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω, —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(all_items)} —Ç–æ–≤–∞—Ä–æ–≤.')
                update_config_status(db_connection, "parser_status", "done")
                update_config_status(db_connection, "parser_update_time", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            


            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")
                update_config_status(db_connection, "parser_status", "failed")


if __name__ == '__main__':
    main()
